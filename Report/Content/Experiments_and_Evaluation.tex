\section{Experiments \& Evaluation}

This section will benchamrk the performance of Spiral against the coulouring and PBC
datasets of our study.

\subsection{Experimental Setup}

\textbf{Spiral Parameters} \hspace{0.3cm} We run our server and client executables over the same set
of parameters. These parameters are selected through the automatic parameter
selection\cite{1}, and is determined before the server and client are executed.

\textbf{Hardware} \hspace{0.3cm} Evaluation of Spiral is performed over a \texttt{M5zn} AWS
instance. Our configuration followed 32 vCPUs and 128GiB of memory. \texttt{AVX512F} and \texttt{AVX2}
support was enabled, and the instance was running on Ubuntu 20.04. We used \texttt{Clang
12} and compiled with \texttt{O3} optimisations.

\textbf{Trials} \hspace{0.3cm} We run our trials over database sizes corresponding to the
following values for $q$ and $h$:

\begin{center}
    \begin{tabular}{ |c|c| }
        \hline
        $q$ & $h$ \\
        \hline
        $2$   & $10...24$ \\
        $16$  & $12, 16, 20, 24$ \\
        $126$ & $14, 21$ \\
        $256$ & $16, 24$ \\
        \hline
    \end{tabular}
\end{center}

Trials are ran over a pre-defiend indices file generated by the colouring and PBC
algorithms. These set of indices are a realistic depeiction of how hashes will be queried
in a realistic implementation.

\textbf{Measurements} \hspace{0.3cm} Time measurements are taken using the
\texttt{std::chrono::high\_resolution\_clock}. The average, minimum and maximum are
recorded for each metric. These averages are taken over the number of indices specified in
a single indices file.

\subsection{Metrics}

We use the following metrics to evaluate the performance of Spiral:

\itemize
\item \textbf{Database-Dependant Compuation} The time taken by the server to
process the query against the database and produce a result.
\item \textbf{Database Generation} The time required to load and encoded a database of
size $N$ into memory on the server.
\item \textbf{Query Generation} The time taken to generate a client-side query.
\item \textbf{Response Extraction} The time taken to decode and extract the requested hash
from the server response on the client.
\item \textbf{Rate} The ratio of the response size to the size of the retrieved record.
The rate stands to measure the overehead in the server-client communication.
\item \textbf{Query Size} The size of the encoded query sent by the client to the server.
\item \textbf{Response Size} The size of the received encoded response from the server.
\item \textbf{Total Cost} The addtion of the query and response sizes.

\subsection{Evaluation Results}

\subsubsection{Colouring}

\input{Images/Evaluation/Colouring/Latex_Tables/Answer_time.tex}
\newpage
\input{Images/Evaluation/Colouring/Latex_Tables/Encoding_time.tex}
\newpage
\input{Images/Evaluation/Colouring/Latex_Tables/Query_extraction_time.tex}
\newpage
\input{Images/Evaluation/Colouring/Latex_Tables/Query_generation_time.tex}
\newpage
\input{Images/Evaluation/Colouring/Latex_Tables/Rate.tex}
\newpage
\input{Images/Evaluation/Colouring/Latex_Tables/Query_cost.tex}
\newpage
\input{Images/Evaluation/Colouring/Latex_Tables/Response_cost.tex}
\newpage
\input{Images/Evaluation/Colouring/Latex_Tables/Total_cost.tex}

\subsubsection{PBC}

\input{Images/Evaluation/PBC/Latex_Tables/Answer_time.tex}
\newpage
\input{Images/Evaluation/PBC/Latex_Tables/Encoding_time.tex}
\newpage
\input{Images/Evaluation/PBC/Latex_Tables/Query_extraction_time.tex}
\newpage
\input{Images/Evaluation/PBC/Latex_Tables/Query_generation_time.tex}
\newpage
\input{Images/Evaluation/PBC/Latex_Tables/Rate.tex}
\newpage
\input{Images/Evaluation/PBC/Latex_Tables/Query_cost.tex}
\newpage
\input{Images/Evaluation/PBC/Latex_Tables/Response_cost.tex}
\newpage
\input{Images/Evaluation/PBC/Latex_Tables/Total_cost.tex}

\newpage

\subsection{Discussion}

It should be noted that the maximum database evaluated for Spiral in the original
paper\cite{1} was $2^{20}$ elements where every element size is $256$ bytes. PIR schemes
are known to struggle with larger databases, and we found the required configuration to
store anytig above $2^{26}$ elements to be too infeasible for most machines. As an
example, a database of around $2^{28}$ elements could take up to $700GiB$ of memory.

Spiral has exhibited expected, similar performance across our colouring and PBC datasets.
This is to be expected given the almost identical nature of the data to be stored.

The \textit{rate} of our trials appears to diminish significantly as the size of the
required database increases. This pattern is also noted in the original paper\cite{1},
with a decrease in rate corresponding to an increase in database size. A possible
explanation for this observation is that encoding more elements into a single database
entry reduces the overall size of the database. Consequently, a smaller database size
leads to quicker database-dependent computations, especially in operations involving first
dimension multiplication and repeated folding across subsequent dimensions.

Given that we have defined three methodologies to encode hashes across three different
plaintext moduli $p$, we can observe the rate to be of three different values: $0.031$, $0.008$
and $0.004$. The first of which is seen on smaller databases, as a single hash takes up a
larger portion of the database record. Likewise, as the value of $p$ increases, we are
able to pack more data into a single coeffiecient of the ring polynomial, and thus the
rate decreases as more hashes are represented in a record.

The response size provided by the server follows a similar pattern to the rate, with the
sizes gradually increasing as the $p$ value increases. Again, this could be explained by
the necessity to encode more hashes into a single record in the database. This retrieved
record is what gets sent back, and thus the response size increases as the rate decreases.

Unlike the response size, the query size stays constant throughout all trials. This is
because the encoding of the hash index to retrieve follows a consistent approach
irrespective of the database size.

The time required to extract the requested hash from the server response is also seen to
slightly increase alongside the database size. This is likely due to the fact that the
encoding methodolgies for larger database sizes are more complex, and may therefore
require more time to decode the hash from the server response.
